{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rc377MOGGeaD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "df = pd.read_csv('crimeTime.csv', engine='python', on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install panda and numpy library\n",
        "\n",
        "<br>**sklearn.model_selection** for splitting data into training/testing sets\n",
        "<br>**sklearn.preprocessing** includes OneHotEncoder which just turns categorical variables into numeric features\n",
        "<br>**sklearn.metrics** to measure accuracy\n",
        "<br>**tensorflow.keras** is a framework for building models, MLP in my case\n"
      ],
      "metadata": {
        "id": "KY8VbYLqGg3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#one-hot encode 'Category' (crime type) for classification\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "category_encoded = encoder.fit_transform(df[['Category']])\n",
        "category_labels = encoder.categories_[0]\n",
        "\n",
        "#create dataframe for encoded labels\n",
        "category_df = pd.DataFrame(category_encoded, columns=category_labels)"
      ],
      "metadata": {
        "id": "dAhwrXhjGhGv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step transforms the \"Category\" column into a group of binary columns, one for every unique category.\n",
        "<br> The model handles the different crime types as separate numeric inputs."
      ],
      "metadata": {
        "id": "CvWT3Gi3GyOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#one-hot encode 'ordinalDistrict'\n",
        "district_encoder = OneHotEncoder(sparse_output=False)\n",
        "district_encoded = district_encoder.fit_transform(df[['ordinalDistrict']])\n",
        "district_labels = district_encoder.categories_[0]\n",
        "\n",
        "#create dataframe for encoded districts\n",
        "district_df = pd.DataFrame(district_encoded, columns=[f'District_{int(d)}' for d in district_labels])"
      ],
      "metadata": {
        "id": "2ZVPoKxsGi2b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exactly the same as the previous step, but this time it transforms the \"PdDistrict\" column."
      ],
      "metadata": {
        "id": "TmYHSR9FGi7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merge with main dataframe\n",
        "df = pd.concat([df[['ordinalDOW', 'Time_Minutes']], district_df, category_df], axis=1)\n",
        "\n",
        "#normalize 'Time_Minutes'\n",
        "df['Time_Minutes'] = df['Time_Minutes'] / 1440 #1440 minutes = 1 day\n",
        "\n",
        "#split dataset into features (X) and target (y)\n",
        "X = df.drop(category_labels, axis=1)\n",
        "y = category_encoded\n",
        "\n",
        "#train-test split (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "lZymtcJGGjBP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df = pd.concat()** combines the original dataframe columns with the new dataframe columns created from the previous two steps.\n",
        "\n",
        "<br>Time_Minutes are then normalized into a 0-1 range, divided by 1440  because 1440minutes=1day\n",
        "\n",
        "<br>The dataset is then split between features and the target.\n",
        "\n",
        "<br>The dataset is then split into two sets, one for training and one for testing. 70/30 split"
      ],
      "metadata": {
        "id": "wB3pLAf2GjFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define MLP architecture for classification\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "Pf3RFjzVGjKH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step creates the multi-layer perceptron (MLP) with:\n",
        "\n",
        "*   **Input Layer** that recieves an input = # of features in the training model\n",
        "*   **3 Hidden Layers** (128 -> 64 -> 32 neurons), each followed by a batch norm and dropout\n",
        "    *   Batch Normalization normalizes the outputs from the previous layer (faster training)\n",
        "    *   Dropout randomly turns off 30% of the neurons during training (helps prevents overfitting)\n",
        "*   **A final Dense Layer** that outputs the probability for each crime category\n",
        "*   ReLu which is an activation function that helps the model train better since it is only comparing with 0\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CVOaHLgRGjOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model with a smaller learning rate\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "#train the model\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=1, #set to 1 for demonstration purposes because each pass through could take minutes\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CjmNEzxgGjTX",
        "outputId": "e47d2144-60e1-4408-e3b0-17bf87a3c6f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37267/37267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 5ms/step - accuracy: 0.2346 - loss: 2.5798 - val_accuracy: 0.2422 - val_loss: 2.5393\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b83b691ab10>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Adam (Adaptive Moment Estimation) optimizer, it adjusts the learning rate for each parameter.\n",
        "\n",
        "<br>The model is then compiled using the categorical_crossentrophy loss function\n",
        "*   specifically because it is suitable for multi-class classifications and it tries to minimize the differences between the predicted probabilities and the actual probabilities\n",
        "*   epochs are the # of pass throughs the model takes\n",
        "*   batch size are the  of samples the model processes before updating the weights\n",
        "*   validation split is 20%, meaning  that 80% of the samples are used for training and remaining 20% of the samples are used for performance evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "EHTiEvVTGjXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model on the test data\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "#calculate accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Njf296ZNG6bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final step is to evaluate the model.\n",
        "<br>This compares the predicted class indices with the true class indices, then prints the fraction of correct predictions."
      ],
      "metadata": {
        "id": "l4_TAwixGhYX"
      }
    }
  ]
}
